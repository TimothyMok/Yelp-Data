---
Title: "Yelp Data Analysis"
Student: "Timothy Mok"

output:
  pdf_document: default
---

```{r Library initiation}
  Packages<-c("data.table","NLP","tm","Matrix","SnowballC","crqanlp","qdap","wordcloud","qdapTools","ggplot2","quanteda","rpart","ggthemes","caTools","rpart.plot","glmnet","caret","plotrix")
  lapply(Packages,library,character.only=TRUE)
  rm(Packages)
```

###############################################################################################
#                 1 - Extract, Transform, Load data from it's original source                 #
###############################################################################################

# This Section read load both section of csv that we will use for the data analysis, the data are trim down to 10,000 observation in order to increase the performance of the analysis

```{r Extraction of data from CSV files, dimension reduction}
    business<-read.csv("yelp_business.csv",stringsAsFactors = FALSE)
    business<-business[c(1:10000),]
    review<-read.csv("yelp_review.csv",stringsAsFactors = FALSE)
    review<-review[c(1:10000),]
    qplot(data=review,stars)
```

```{r Data Observation & Column manipulation}
  # Review all attributes
    str(review)
    str(business)
  # Attributes trim and review new 
    review<-review[c("stars","text")]
    business<-business[c("name","city","stars","review_count")]
    str(review)
    str(business)
  # Removes neutral stars comment(3 stars) and include a factor to distinguish positive(4 or more stars) and negative(2 or less     stars)
    review<-subset(review,stars!=3)
    review$positive<-as.factor(review$stars>3)
  # Create a positive and negative Corpus
    reviewP<-subset(review,positive==TRUE)
    reviewP<-reviewP["text"]
    reviewN<-subset(review,positive==FALSE)
    reviewN<-reviewN["text"]
```

```{r Corpus creation & text procesisng}
  Corpus<-Corpus(VectorSource(review$text))
  func<-list(removePunctuation,tolower,removeNumbers, stripWhitespace)
  Corpus<-tm_map(Corpus,FUN=tm_reduce,tmFuns=func)
  Corpus<-tm_map(Corpus, removeWords, stopwords('english'))
  Corpus<-tm_map(Corpus, stemDocument)
# Verify the text have been process successfully
  Corpus[[5]][1]
```

###############################################################################################
#                3 - Create a Term Document Matrix & Document Term Matrix                     #
###############################################################################################

```{r Creates a DTM and TDM for analsis with text Corpus}
    CorpusDTM<-DocumentTermMatrix(Corpus)
    CorpusTDM<-TermDocumentMatrix(Corpus)
```

###############################################################################################
#                    4 - Exploratory analysis on  data, text & visualization                  #
###############################################################################################


```{r Find the frequent terms in the corpus, 30 terms}
  TermFreq<-freq_terms(Corpus,30)
  plot(TermFreq)
```

```{r Find the top appearing words using the TDM Matrix}
  CorpusMatrix<-as.matrix(CorpusTDM)
  CorpusTotal<-rowSums(CorpusMatrix)
  CorpusSorted<-sort(CorpusTotal,decreasing=TRUE)
  as.matrix(CorpusSorted[1:30])
```

```{r Explore the top words with bar plot}
  barplot(CorpusSorted[1:30],ylim=c(0,6000),col="orange",las=3,xlab="Word",ylab="Frequencies")
```

```{r Simple words clustering & dentrogram}
  CorpusTDMx<-removeSparseTerms(CorpusTDM,sparse=0.86)
  dentro<-hclust(d=dist(CorpusTDMx, method = "euclidean"),method="complete")
  plot(dentro)
```

```{r Create bi-grams from Corpus}
  bigram <- tokens(review$text)%>%
    tokens_remove("\\p{P}",valuetype="regex",padding=TRUE)%>%
    tokens_remove(stopwords("english"),padding=TRUE)%>%
    tokens_ngrams(n = 2)%>%
    dfm()
  topfeatures(bigram)
```

```{r Pyramaid plot for negative and positive comments, common words comparison}
  # Combining Positive and Negative comments from the original Corpus
    Pos<-paste(reviewP,collapse="")
    Neg<-paste(reviewN,collapse="")
    All<-c(Pos,Neg)
  # Text process/TDM creation and include +/- coloumns
    All<-Corpus(VectorSource(All)) 
    All<-tm_map(All, tolower)
    All<-tm_map(All, removePunctuation)
    All<-tm_map(All, removeWords, stopwords("english"))
    All<-tm_map(All, removeWords,c("also", "get","like", "company", "made", "can", "im", "dress","just","i"))
    All<-tm_map(All, stemDocument)
    AllTDM<-TermDocumentMatrix(All)
    AllMatrix<-as.matrix(AllTDM)
    colnames(AllMatrix)<-c("Positive","Negative")
    
    comparison.cloud(AllMatrix,colors=c("green","red"),max.words=40)
  
  # Identifying terms shared by both documents
    Common<-subset(AllMatrix, AllMatrix[,1] > 0 & AllMatrix[,2]>0)
  # Calculate common & difference words
    Difference<-abs(Common[,1]-Common[,2])
    Common<-cbind(Common,Difference)
    Common<-Common[order(Common[, 3],decreasing=TRUE),]
    head(Common,10)
  # Create DF for positive and negative count
    TopDF<-data.frame(x=Common[1:25, 1],
                      y=Common[1:25, 2],
                      labels=rownames(Common[1:25, ]))
  # Creating the pyramid plot
    pyramid.plot(TopDF$x,TopDF$y,
                 labels=TopDF$labels, 
                 main="Common Words in Negative and Positive Review",
                 gap=2000,
                 laxlab=NULL,
                 raxlab=NULL, 
                 unit=NULL,
                 top.labels=c("Positive",
                              "Words",
                              "Negative")
                 )
```

###############################################################################################
#                    5 - Remove sparsity from TDM/splitting train/testing data               #
###############################################################################################

```{r Remove Sparsity}
  Sparse<-removeSparseTerms(CorpusDTM, 0.99)
  Sparse<-as.data.frame(as.matrix(Sparse))
  colnames(Sparse)<-make.names(colnames(Sparse))
  Sparse$positive<-review$positive
```

```{r Data split, 70 Training 30 Testing}
  set.seed(168)
  split<-sample.split(Sparse$positive,SplitRatio=0.7)
  Sparse$split<-split
  train<-subset(Sparse,split==TRUE)
  test<-subset(Sparse,split==FALSE)
```

```{r table for baseline accuracy to surpass}
  table(train$positive)
  4564/nrow(train)
```

###############################################################################################
#                               6 - Build the classification Model                            #
###############################################################################################

```{r Cart Model & plot}
  CART<-rpart(positive~.,data=train,method="class")
  prp(CART)
```

```{r}
  predictCART<-predict(CART,newdata=test,type="class")
  table(test$positive, predictCART)
  
  (188+1867)/nrow(test)
```

```{r}
  Num=trainControl(method="cv",number=10)
  cpGrid = expand.grid(.cp=seq(0.01,0.1,0.01))
  train(positive~.,data=train,method="rpart",trControl=Num,tuneGrid=cpGrid)
  
  cartModelImproved = rpart(positive~.,data=train,method="class", cp= 0.003)
  prp(cartModelImproved)
```

###########################################################

