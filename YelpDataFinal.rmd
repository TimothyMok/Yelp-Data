---
Student: Timothy Mok
Title: Yelp Data Analysis
output:
  pdf_document: default
  word_document: default
---

```{r Library initiation}
  Packages<-c("data.table","NLP","tm","Matrix","SnowballC","crqanlp","qdap","wordcloud","qdapTools","ggplot2","quanteda","rpart","ggthemes","caTools","rpart.plot","glmnet","caret","plotrix","randomForest")
  lapply(Packages,library,character.only=TRUE)
  rm(Packages)
```

###############################################################################################
#                 1 - Extract, Transform, Load data from it's original source                 #
###############################################################################################

# This Section read load both section of csv that we will use for the data analysis, the data are trim down to 10,000 observation in order to increase the performance of the analysis

```{r Extraction of data from CSV files, dimension reduction}
    business<-read.csv("yelp_business.csv",stringsAsFactors = FALSE)
    business<-business[c(1:10000),]
    review<-read.csv("yelp_review.csv",stringsAsFactors = FALSE)
    review<-review[c(1:10000),]
    qplot(data=review,stars)
```

```{r Data Observation & Column manipulation}
  # Review all attributes
    str(review)
    str(business)
  # Attributes trim and review new 
    review<-review[c("stars","text")]
    business<-business[c("name","city","stars","review_count")]
    str(review)
    str(business)
  # Removes neutral stars comment(3 stars) and include a factor to distinguish positive(4 or more stars) and negative(2 or less     stars)
    review<-subset(review,stars!=3)
    review$positive<-as.factor(review$stars>3)
  # Create a positive and negative Corpus
    reviewP<-subset(review,positive==TRUE)
    reviewP<-reviewP["text"]
    reviewN<-subset(review,positive==FALSE)
    reviewN<-reviewN["text"]
```

```{r Corpus creation & text procesisng}
  Corpus<-Corpus(VectorSource(review$text))
  func<-list(removePunctuation,tolower,removeNumbers, stripWhitespace)
  Corpus<-tm_map(Corpus,FUN=tm_reduce,tmFuns=func)
  Corpus<-tm_map(Corpus, removeWords, stopwords('english'))
  Corpus<-tm_map(Corpus, stemDocument)
# Verify the text have been process successfully
  Corpus[[5]][1]
```

###############################################################################################
#                2 - Create a Term Document Matrix & Document Term Matrix                     #
###############################################################################################

```{r Creates a DTM and TDM for analsis with text Corpus}
    CorpusDTM<-DocumentTermMatrix(Corpus)
    CorpusTDM<-TermDocumentMatrix(Corpus)
```

###############################################################################################
#                    3 - Exploratory analysis on  data, text & visualization                  #
###############################################################################################


```{r Find the frequent terms in the corpus, 30 terms}
  TermFreq<-freq_terms(Corpus,30)
  plot(TermFreq)
```

```{r Find the top appearing words using the TDM Matrix}
  CorpusMatrix<-as.matrix(CorpusTDM)
  CorpusTotal<-rowSums(CorpusMatrix)
  CorpusSorted<-sort(CorpusTotal,decreasing=TRUE)
  CorpusSorted[1:30]
```

```{r Explore the top words with bar plot}
  barplot(CorpusSorted[1:30],ylim=c(0,6000),col="orange",las=3,xlab="Word",ylab="Frequencies")
```

```{r Simple words clustering & dentrogram}
  CorpusTDMx<-removeSparseTerms(CorpusTDM,sparse=0.86)
  dentro<-hclust(d=dist(CorpusTDMx, method = "euclidean"),method="complete")
  plot(dentro)
```

```{r Create bi-grams from Corpus}
  bigram <- tokens(review$text)%>%
    tokens_remove("\\p{P}",valuetype="regex",padding=TRUE)%>%
    tokens_remove(stopwords("english"),padding=TRUE)%>%
    tokens_ngrams(n = 2)%>%
    dfm()
  topfeatures(bigram)
```

```{r Pyramaid plot for negative and positive comments, common words comparison}
  # Combining Positive and Negative comments from the original Corpus
    Pos<-paste(reviewP,collapse="")
    Neg<-paste(reviewN,collapse="")
    All<-c(Pos,Neg)
  # Text process/TDM creation and include +/- coloumns
    All<-Corpus(VectorSource(All)) 
    All<-tm_map(All, tolower)
    All<-tm_map(All, removePunctuation)
    All<-tm_map(All, removeWords, stopwords("english"))
    All<-tm_map(All, removeWords,c("also", "get","like", "company", "made", "can", "im", "dress","just","i"))
    All<-tm_map(All, stemDocument)
    AllTDM<-TermDocumentMatrix(All)
    AllMatrix<-as.matrix(AllTDM)
    colnames(AllMatrix)<-c("Positive","Negative")
    
    comparison.cloud(AllMatrix,colors=c("green","red"),max.words=40)
  
  # Identifying terms shared by both documents
    Common<-subset(AllMatrix, AllMatrix[,1] > 0 & AllMatrix[,2]>0)
  # Calculate common & difference words
    Difference<-abs(Common[,1]-Common[,2])
    Common<-cbind(Common,Difference)
    Common<-Common[order(Common[, 3],decreasing=TRUE),]
    head(Common,10)
  # Create DF for positive and negative count
    TopDF<-data.frame(x=Common[1:25, 1],
                      y=Common[1:25, 2],
                      labels=rownames(Common[1:25, ]))
  # Creating the pyramid plot
    pyramid.plot(TopDF$x,TopDF$y,
                 labels=TopDF$labels, 
                 main="Common Words in Negative and Positive Review",
                 gap=700,
                 laxlab=NULL,
                 raxlab=NULL, 
                 unit=NULL,
                 top.labels=c("Positive",
                              "Words",
                              "Negative")
                 )
```

###############################################################################################
#                    4 - Remove sparsity from TDM/splitting train/testing data               #
###############################################################################################

```{r Remove Sparsity}
  SPR<-removeSparseTerms(CorpusDTM, 0.99)
  SPR<-as.data.frame(as.matrix(SPR))
  colnames(SPR)<-make.names(colnames(SPR))
  SPR$positive<-review$positive
```

```{r Data split, 70 Training 30 Testing}
  set.seed(168)
  split<-sample.split(SPR$positive,SplitRatio=0.7)
  SPR$split<-split
  training<-subset(SPR,split==TRUE)
  testing<-subset(SPR,split==FALSE)
```

```{r table for baseline accuracy to surpass}
  table(training$positive)
  4564/nrow(training)
```

###############################################################################################
#                   5 - Building the Model(CART, Random Forest, Lasso)                        #
###############################################################################################

```{r Cart Model & plot}
  CART<-rpart(positive~.,data=training,method="class")
  prp(CART)
```

```{r prediction for CART}
  pCART<-predict(CART,newdata=testing,type="class")
  table(testing$positive, pCART)
  
  (188+1867)/nrow(testing)
```

```{r Improved CART, with more branch}
  Num<-trainControl(method="cv",number=10)
  cpGrid<-expand.grid(.cp=seq(0.01,0.1,0.01))
  train(positive~.,data=training,method="rpart",trControl=Num,tuneGrid=cpGrid)
  
  PlusCART<-rpart(positive~.,data=training,method="class",cp=0.0003)
  prp(PlusCART)
```

```{r prediction for Improved CART}
  PPlusCART<-predict(PlusCART, newdata=testing, type="class")
  table(testing$positive, PPlusCART)

  (272+1795)/nrow(testing)
```

```{r Random Forest Model}
RFtext<-randomForest(positive~.,data=testing,ntree=1)
varImpPlot(RFtext,cex=.7)
```

```{r Random Forest accuracy}
  PRFtext<-predict(RFtext,newdata=testing,type="class")
  table(testing$positive, PRFtext)

  (396+1837)/nrow(testing)
```


```{r Lasso Logistic Regression - Cross Validation}
  x<-model.matrix(positive~.,SPR)
  y<-as.numeric(SPR$positive)
  out<-cv.glmnet(x,y,alpha=1,family="binomial",type.measure = "class")
  plot(out)
```

```{r Create Lambda Coef to file}
  lambdamin<-out$lambda.min
  lambda1se<-out$lambda.1se
  lambda1se
  coef=coef(out,s=lambda1se)
  lassocoef=as.matrix(coef(out,s=lambda1se))
  write.csv(lassocoef,"lassocoef.csv")
```

```{r Finding the best lambda using cross-validation}
  set.seed(123) 
  cv.lasso<-cv.glmnet(x,y,alpha=1,family="binomial")
  review_logreg <- glmnet(x, y, alpha= 1, family = "binomial",lambda = cv.lasso$lambda.min)
  logregcoef=as.matrix(coef(review_logreg))
  odds_ratio=as.matrix(exp(coef(review_logreg)))
  write.csv(logregcoef,"logregcoef.csv")
  write.csv(odds_ratio,"oddsratio.csv")
```

